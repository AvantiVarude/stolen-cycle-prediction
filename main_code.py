# -*- coding: utf-8 -*-
"""code_for_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rg0bJiVO_a219hR0OGdJYMNpYO475QHd

#Machine Learning Project: Lost Cycle Prediction
"""

#Mount google drive for dataset
from google.colab import drive
drive.mount('/content/drive')

#Import libraries
# !pip install pandas
!pip install category_encoders
import category_encoders as ce
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/ML_project/sheet.csv", index_col=False)

"""###Data Preprocessing and cleaning"""

df.head()

df.isnull().sum()

df=df.dropna(subset = ['Stolen','HostelCategory','Cycle_Name', 'OwnerHostel'])

df['Cycle_Name'] = df['Cycle_Name'].str.lower()
df['Cycle_company'] = df['Cycle_company'].str.lower()

#Create seperate dataset for null and non null values of price 
df_non_null=df.dropna(subset=['Price'])
bool_series = pd.isnull(df["Price"])
df_null=df[bool_series]

#Reset index of datasets
df_null = df_null.reset_index()
df_non_null = df_non_null.reset_index()
df_null = df_null.drop(['index'], axis=1)
df_non_null = df_non_null.drop(['index'], axis=1)

#fill null values of price in df_null dataset which has same company name and model name in df_non_null dataset
for i in range(len(df_null)):
  for j in range(len(df_non_null)):
    if(df_null['Cycle_company'][i]==df_non_null['Cycle_company'][j] and df_null['Cycle_Name'][i]==df_non_null['Cycle_Name'][j]):
     df_null['Price'][i]=df_non_null['Price'][j]

#Merge df_null and df_non_null datasets
new_df=pd.concat([df_null, df_non_null])

#reset index of new_df
new_df = new_df.reset_index()
new_df = new_df.drop(['index'], axis=1)

new_df.isnull().sum()

new_df.head()

#Fill remaining null values of dataset with median of prices
new_df['Price'] = new_df['Price'].fillna(new_df['Price'].median())

#Convert all data into lowercase
new_df = new_df.apply(lambda x: x.astype(str).str.lower())

new_df.head()

"""###Data infographics"""

# !pip install seaborn
import seaborn as sns
from collections import Counter
data = Counter(new_df["OwnerHostel"])
plt.pie(new_df["OwnerHostel"].value_counts(), labels = new_df["OwnerHostel"].unique(), autopct='%.0f%%')
plt.show()

data = Counter(new_df["HostelCategory"])
plt.pie(new_df["HostelCategory"].value_counts(), labels = new_df["HostelCategory"].unique(), autopct='%.0f%%')
plt.show()

plt.hist(new_df["Price"], bins=len(new_df["Price"].value_counts())//2)
plt.show()

#Seperate label and feature colums
X = new_df.iloc[:, :-1]
y = new_df.iloc[:, -1]

X.describe()

y.describe()

#Convert all yes and no valus into 0-1 format(yes=1,no=0)
# y.unique()
for i in range(len(y)):
  if y[i].strip().lower() == 'yes':
    y[i] = 1
  else:
    y[i] = 0

y.unique()

cat_df = X.select_dtypes(include=['object']).copy()
cat_df.head()

#Convert all object data into binary format useing encoding
encoder = ce.BinaryEncoder()
df_binary = encoder.fit_transform(cat_df)
df_binary.head()

numeric_df = X.select_dtypes(include=['int', 'float']).copy()
numeric_df.head()

# normalized_df = numeric_df.copy()
for column in numeric_df.columns:
    numeric_df[column] = (numeric_df[column] -
                           numeric_df[column].mean()) / numeric_df[column].std()

#concatenate numerical and categorical df
final_df = pd.concat([numeric_df, df_binary], axis = 1)
final_df.head()

"""##Logistic Regression from scratch

Import Libraries
"""

import sys
sys.version

# !pip3 install Pillow
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import confusion_matrix

#Split data into training and testing
X=final_df
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0, stratify=y)

samples,features=X_train.shape

"""###Supporting Functions"""

#Sigmoid activation function
def sigmoid_function(p):
  a=1/(1+np.exp(-p))
  return a

def initialization(Data_X):
  #initialize weights and bias
  Weights = np.zeros(features)
  bias = 0
  return Weights, bias

def forward_prop(Data_X,Weights,bias):
  linear_function1 = np.dot(Data_X, Weights) + bias
  Activation = sigmoid_function(linear_function1)
  return Activation

def gradient_calc(Activation,Data_Y):
  #calculation for gradient descent
  deriv_Z = Activation - Data_Y
  deriv_W = (1 / samples) * deriv_Z.dot(Activation.T)
  deriv_b = (1 / samples) * np.sum(deriv_Z)
  return deriv_W,deriv_b

def gradient_descent(Weights,bias,lr,deriv_W,deriv_b):
  #Update parameters
  Weights = Weights - lr * deriv_W
  bias = bias - lr * deriv_b
  return Weights, bias

#Function to calculate loss
def loss_calculation(y_pred, Data_Y):
  m = Data_Y.shape[0]
  log_array_probs = np.multiply(np.log(y_pred),Data_Y)+np.multiply(np.log(1-y_pred),(1-Data_Y))
  loss = -1/m*np.sum(log_array_probs)
  loss = float(np.squeeze(loss))
  return loss

#Function to calculate accuracy
def accuracy_calculation(y_pred, Data_Y, Weights, bias):
    y_pred = y_pred > 0.3
    accuracy=np.sum(y_pred == Data_Y) / Data_Y.size
    return y_pred ,accuracy

#Main logistic regression function
def logistic_regression(Data_X,Data_Y,lr,iter):
  accuracy_array = []
  loss = []
  Weights, bias=initialization(Data_X)
  for i in range(iter):
    y_pred=forward_prop(Data_X,Weights,bias)
    deriv_W,deriv_b=gradient_calc(y_pred,Data_Y)
    Weights, bias=gradient_descent(Weights,bias,lr,deriv_W,deriv_b)
    
    print("\niter: ", i)
    
    #Calculate loss
    cost=loss_calculation(y_pred,Data_Y)
    print("Loss",cost)
    loss.append(cost)
    
    #Calculate accuracy
    y_predict,accuracy=accuracy_calculation(y_pred, Data_Y, Weights, bias)
    print("Accuracy: ",accuracy)
    accuracy_array.append(accuracy)
  return Weights, bias, loss, accuracy_array, y_predict

"""###Driver code"""

#main driver code
lr=0.05
iter=200
print("\nFor parameters: learning rate = %f; iteration = %d\n" % (0.05,200))
Weights, bias, loss, accuracy_array, y_predict = logistic_regression(X_train, y_train, lr, iter)

"""###Results"""

#plot iteration vs loss graph
plt.plot(np.arange(iter),loss)
plt.title("iter Vs Loss")
plt.xlabel("iter")
plt.ylabel("Loss")
plt.show()

#plot iteration vs accuracy graph
plt.plot(np.arange(iter),accuracy_array)
plt.title("iter Vs Accuracy")
plt.xlabel("iter")
plt.ylabel("Accuracy")
plt.show()

"""###Metrics functions"""

"""#### Metrics functions"""

def true_negative(y_actual, y_pred):
    true_neg = 0
    for gt, pred in zip(y_actual, y_pred):
        if gt == 0 and pred == 0:
            true_neg +=1
    return true_neg
    
def true_positive(y_actual, y_pred):
    true_pos = 0
    for gt, pred in zip(y_actual, y_pred):
        if gt == 1 and pred == 1:
            true_pos +=1
    return true_pos
    
def false_positive(y_actual, y_pred):
  false_pos = 0
  for gt, pred in zip(y_actual, y_pred):
      if gt == 0 and pred == 1:
          false_pos +=1
  return false_pos

def false_negative(y_actual, y_pred):
    false_neg = 0
    for gt, pred in zip(y_actual, y_pred):
        if gt == 1 and pred == 0:
            false_neg +=1
    return false_neg

def accuracy_result(y_actual, y_pred):
    true_pos = true_positive(y_actual, y_pred)  
    false_pos = false_positive(y_actual, y_pred)  
    false_neg = false_negative(y_actual, y_pred)  
    true_neg = true_negative(y_actual, y_pred)  
    acc = (true_pos + true_neg)/ (true_pos + true_neg + false_pos + false_neg)  
    return acc

def precision(y_actual, y_pred):
    true_pos = true_positive(y_actual, y_pred)  
    false_pos = false_positive(y_actual, y_pred)  
    prec = true_pos/ (true_pos + false_pos)  
    return prec

def recall(y_actual, y_pred):
    true_pos = true_positive(y_actual, y_pred)  
    false_neg = false_negative(y_actual, y_pred)  
    rec = true_pos/ (true_pos + false_neg)  
    return rec

def f1_score(y_actual, y_pred):
    p = precision(y_actual, y_pred)
    r = recall(y_actual, y_pred)
    f1 = 2 * p * r/ (p + r) 
    return f1

def print_metrics(y_pred, y_test):
  print("Accuracy: ", accuracy_result(y_test, y_pred))
  print("Precision: ", precision(y_test, y_pred))
  print("Recall: ", recall(y_test, y_pred))
  print("F1: ", f1_score(y_test, y_pred))

#confusion matrix on training samples
y_predict=np.round(y_predict)
y_predict = y_predict.astype(int)
y_train_int=np.array(y_train)
y_train_int=y_train_int.astype(int)
confusion_mat = confusion_matrix(y_train_int, y_predict)
print ("Confusion Matrix : \n", confusion_mat)

#Accuracy on test dataset
act1=forward_prop(X_test,Weights,bias)
y_predict,accuracy=accuracy_calculation(act1,y_test,Weights,bias)

#confusion matrix on training samples
y_predict=np.round(y_predict)
y_predict = y_predict.astype(int)
y_test_int=np.array(y_test)
y_test_int=y_test_int.astype(int)
confusion_mat = confusion_matrix(y_test_int, y_predict)
print ("Confusion Matrix : \n", confusion_mat)

#results
print_metrics(y_predict,y_test_int)

